{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('..')\n",
    "sys.path.insert(0, './node2vec/') # use local node2vec, modified to allow temporal walk option\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "from utils import read_graph, remove_random_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    \n",
    "    def __init__(self, quiet=True, print_last=False, plot=True):\n",
    "        self.epoch = 0\n",
    "        self.previous_running_sum = 0\n",
    "        self.losses = []\n",
    "        self.quiet = quiet\n",
    "        self.print_last = print_last\n",
    "        self.plot = plot\n",
    "    \n",
    "    def on_epoch_begin(self, model):\n",
    "        if not self.quiet:\n",
    "            print(\"Epoch #{} start\".format(self.epoch))\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "        next_running_sum = model.get_latest_training_loss()\n",
    "        loss = next_running_sum - self.previous_running_sum\n",
    "        if not self.quiet:\n",
    "            print(\"Epoch #{} end -- loss = {}\".format(self.epoch, loss))\n",
    "        self.epoch += 1\n",
    "        self.previous_running_sum = next_running_sum\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "    def on_train_end(self, model):\n",
    "        if self.print_last:\n",
    "            print(\"final loss: {}\".format(self.losses[-1]))\n",
    "        if self.plot:\n",
    "            plt.plot(self.losses)\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"training loss\")\n",
    "            plt.title(\"Training loss over epochs\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch for $p$ and $q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Computing transition probabilities:   0%|          | 0/3783 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   0%|          | 12/3783 [00:00<00:31, 119.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   1%|          | 28/3783 [00:00<00:29, 126.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   2%|▏         | 62/3783 [00:00<00:23, 155.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   3%|▎         | 100/3783 [00:00<00:19, 188.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   4%|▍         | 149/3783 [00:00<00:15, 230.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   6%|▌         | 214/3783 [00:00<00:12, 282.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:   8%|▊         | 285/3783 [00:00<00:10, 344.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  10%|█         | 389/3783 [00:00<00:07, 429.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  12%|█▏        | 454/3783 [00:00<00:07, 446.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  14%|█▎        | 514/3783 [00:01<00:07, 437.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  17%|█▋        | 634/3783 [00:01<00:05, 539.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  21%|██        | 800/3783 [00:01<00:04, 676.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  25%|██▍       | 945/3783 [00:01<00:03, 805.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  31%|███       | 1175/3783 [00:01<00:02, 999.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  35%|███▍      | 1324/3783 [00:01<00:02, 823.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  38%|███▊      | 1446/3783 [00:02<00:03, 672.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  41%|████      | 1546/3783 [00:02<00:03, 657.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  43%|████▎     | 1635/3783 [00:02<00:03, 686.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  45%|████▌     | 1720/3783 [00:02<00:03, 614.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  47%|████▋     | 1795/3783 [00:02<00:03, 537.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  49%|████▉     | 1860/3783 [00:02<00:03, 540.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  51%|█████     | 1931/3783 [00:02<00:03, 567.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  53%|█████▎    | 2015/3783 [00:02<00:02, 625.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  55%|█████▌    | 2084/3783 [00:03<00:02, 603.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  57%|█████▋    | 2156/3783 [00:03<00:02, 633.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  60%|█████▉    | 2252/3783 [00:03<00:02, 704.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  62%|██████▏   | 2328/3783 [00:03<00:02, 682.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  63%|██████▎   | 2400/3783 [00:03<00:02, 538.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  65%|██████▌   | 2462/3783 [00:03<00:02, 511.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  67%|██████▋   | 2549/3783 [00:03<00:02, 581.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  71%|███████   | 2683/3783 [00:03<00:01, 698.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  73%|███████▎  | 2769/3783 [00:04<00:01, 715.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  75%|███████▌  | 2852/3783 [00:04<00:01, 720.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  78%|███████▊  | 2932/3783 [00:04<00:01, 736.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  80%|███████▉  | 3015/3783 [00:04<00:01, 749.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  83%|████████▎ | 3121/3783 [00:04<00:00, 790.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  85%|████████▍ | 3205/3783 [00:04<00:00, 799.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  90%|█████████ | 3411/3783 [00:04<00:00, 977.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities:  96%|█████████▋| 3644/3783 [00:04<00:00, 1181.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Computing transition probabilities: 100%|██████████| 3783/3783 [00:04<00:00, 778.38it/s] \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37830\n",
      "2\n",
      "p = 0.5\t q=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-43f1152b13b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpochLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         model = node2vec.fit(sg=1, negative=0, min_count=1, compute_loss=True, callbacks=[epoch_logger], iter=1,\n\u001b[0;32m---> 11\u001b[0;31m                      relational_weighting=True)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting embeddings for train/val... \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/class/cs224w-project/baselines/node2vec/node2vec/node2vec.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **skip_gram_params)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m#    skip_gram_params['rel_weights_by_walk'] = self.rel_weights_by_walk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mskip_gram_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab, relational_weighting, ave_weight, trans_factor)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = read_graph(\"../db/BTCAlphaNet.csv\", temporal=False)\n",
    "test_edges, test_weights = remove_random_edges(G, int(nx.number_of_edges(G)*0.2))\n",
    "val_edges, val_weights = remove_random_edges(G, int(nx.number_of_edges(G)*0.125))\n",
    "best_loss, best_p, best_q = np.inf, -1, -1\n",
    "for p in [0.25, 0.5, 1, 2, 4]:\n",
    "    for q in [0.25, 0.5, 1, 2, 4]:\n",
    "        node2vec = Node2Vec(G, workers=4, p=p, q=q, temporal=False, relational_weighting=False)\n",
    "        print(\"p = {}\\t q={}\".format(p, q))\n",
    "        epoch_logger = EpochLogger(quiet=True, print_last=True, plot=True)\n",
    "        model = node2vec.fit(sg=1, negative=0, min_count=1, compute_loss=True, callbacks=[epoch_logger], iter=1,\n",
    "                     relational_weighting=False)\n",
    "        \n",
    "        print(\"Getting embeddings for train/val... \", end='')\n",
    "        edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "        m = nx.number_of_edges(G)\n",
    "        X_train = np.zeros((m, node2vec.dimensions))\n",
    "        y_train = np.zeros((m))\n",
    "        for i, edge in enumerate(G.edges):\n",
    "            X_train[i] = edges_embs[tuple(map(str,edge))]\n",
    "            y_train[i] = G.get_edge_data(*edge)['weight']\n",
    "            \n",
    "        X_val = np.zeros((len(val_edges), node2vec.dimensions))\n",
    "        for i, edge in enumerate(val_edges):\n",
    "            X_val[i] = edges_embs[tuple(map(str,edge))]\n",
    "        y_val = np.array(val_weights)\n",
    "        print(\"done!\")\n",
    "        \n",
    "        print(\"Fitting SVR...\", end='')\n",
    "        reg = SVR(gamma='scale').fit(X_train, y_train)\n",
    "        print(\"done!\")\n",
    "        \n",
    "        y_pred = reg.predict(X_val)\n",
    "        print(\"Root mean squared error = \", end='')\n",
    "        loss = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        print(loss)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            print(\"New min loss!\")\n",
    "            best_loss = loss\n",
    "            best_p = p\n",
    "            best_q = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-63956d511c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_p' is not defined"
     ]
    }
   ],
   "source": [
    "best_p, best_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = read_graph(\"../db/OTCNet-T.csv\", temporal=True)\n",
    "test_edges, test_weights = remove_random_edges(G, int(nx.number_of_edges(G)*0.1))\n",
    "val_edges, val_weights = remove_random_edges(G, int(nx.number_of_edges(G)*1.0/9))\n",
    "#G.remove_edges_from(val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_edges, test_weights = remove_random_edges(G, int(nx.number_of_edges(G)*(1.0/9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5881/5881 [00:00<00:00, 6541.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58810\n",
      "2\n",
      "Epoch #0 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 end -- loss = 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_edges, new_test_weights = remove_random_edges(G, int(nx.number_of_edges(G)*(3.0/5)))\n",
    "test_edges.extend(new_test_edges)\n",
    "test_weights.extend(new_test_weights)\n",
    "node2vec = Node2Vec(G, weight_key='weight', workers=4, p=1, q=1, temporal=True, relational_weighting=True,\n",
    "                    trans_factor=1)\n",
    "epoch_logger = EpochLogger(quiet=False, print_last=False, plot=True)\n",
    "model = node2vec.fit(sg=1, negative=0, min_count=1, compute_loss=True, callbacks=[epoch_logger], iter=1,\n",
    "                     relational_weighting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "m = nx.number_of_edges(G)\n",
    "X_train = np.zeros((m, node2vec.dimensions))\n",
    "y_train = np.zeros((m))\n",
    "for i, edge in enumerate(G.edges):\n",
    "    X_train[i] = edges_embs[tuple(map(str,edge))]\n",
    "    y_train[i] = G.get_edge_data(*edge)['weight']\n",
    "\n",
    "X_test = np.zeros((len(test_edges), node2vec.dimensions))\n",
    "y_test = np.zeros((len(test_edges)))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = edges_embs[tuple(map(str,edge))]\n",
    "    y_test[i] = test_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SVR(gamma='scale').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.37005002227095124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# Make predictions using the testing set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Root mean squared error: {}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
